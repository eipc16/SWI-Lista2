{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac8bda692c3b0f02ccc575452ef66df3a4b08193edb2b4a0ea8482d7cac1bc35"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['all-exchanges-strings.lc.txt', 'all-orgs-strings.lc.txt', 'all-people-strings.lc.txt', 'all-places-strings.lc.txt', 'all-topics-strings.lc.txt', 'cat-descriptions_120396.txt', 'feldman-cia-worldfactbook-data.txt', 'lewis.dtd', 'README.txt', 'reut2-000.sgm', 'reut2-001.sgm', 'reut2-002.sgm', 'reut2-003.sgm', 'reut2-004.sgm', 'reut2-005.sgm', 'reut2-006.sgm', 'reut2-007.sgm', 'reut2-008.sgm', 'reut2-009.sgm', 'reut2-010.sgm', 'reut2-011.sgm', 'reut2-012.sgm', 'reut2-013.sgm', 'reut2-014.sgm', 'reut2-015.sgm', 'reut2-016.sgm', 'reut2-017.sgm', 'reut2-018.sgm', 'reut2-019.sgm', 'reut2-020.sgm', 'reut2-021.sgm', 'text-extractions.ipynb']\n"
     ]
    }
   ],
   "source": [
    "loaded_documents = []\n",
    "\n",
    "DATA_DIR = \"./reuters/\"\n",
    "EXTENSION = \".sgm\"\n",
    "\n",
    "print(os.listdir(DATA_DIR))\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(EXTENSION):\n",
    "        filename = os.path.join(DATA_DIR, file)\n",
    "    \n",
    "        with open(filename, mode='r', encoding='utf-8', errors='ignore') as data_file:\n",
    "            data = data_file.read()\n",
    "\n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            contents = soup.findAll('body')\n",
    "\n",
    "            for content in contents:\n",
    "                loaded_documents.append(content.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loaded_documents\n",
    "\n",
    "def decontract(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def clean_doc(text):\n",
    "    text = re.sub(r'http\\S+', '', text, flags=re.MULTILINE) # remove links\n",
    "    text = re.sub(r'<.*?>', '', text, flags=re.MULTILINE) # remove http tags\n",
    "    text = decontract(text) #decontract\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove special characters\n",
    "    return text\n",
    "\n",
    "documents = list(map(clean_doc, documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "def wordcount_filter(text):\n",
    "    if text is None:\n",
    "        return False\n",
    "    return len(text.split()) >= 700\n",
    "\n",
    "filtered_documents = list(filter(wordcount_filter, documents))\n",
    "print(len(filtered_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(documents, n):\n",
    "    n_grams = []\n",
    "    for doc in documents:\n",
    "        tokens = [token for token in doc.split(\" \") if token != \"\"]\n",
    "        output = set(ngrams(tokens, n))\n",
    "        n_grams.append(output)\n",
    "    return n_grams\n",
    "\n",
    "def calc_similarity(doc_id, documents, ngrams, top_count):\n",
    "    similarity_map = {}\n",
    "\n",
    "    ngrams_for_document = ngrams[doc_id]\n",
    "\n",
    "    # calc Jaccard Similarity\n",
    "    for i in range(len(documents)):\n",
    "        if i != doc_id:\n",
    "            ngrams_for_other_doc = ngrams[i]\n",
    "            jaccard_similarity = len(ngrams_for_document.intersection(ngrams_for_other_doc)) / (len(ngrams_for_document.union(ngrams_for_other_doc)))\n",
    "            similarity_map[i] = jaccard_similarity*100\n",
    "    \n",
    "    # Extract top\n",
    "    sorted_similarity_map = sorted(similarity_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_similarity_map[:top_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best index: 173. Related documents: [(175, 86.89740420271941), (183, 3.6036036036036037), (198, 2.172338884866039), (125, 2.072192513368984), (62, 2.030456852791878), (197, 1.6869095816464237), (109, 1.6284233900814211), (148, 1.5460295151089247), (102, 1.4522821576763485)]\n"
     ]
    }
   ],
   "source": [
    "calculated_ngrams = generate_ngrams(filtered_documents, 3)\n",
    "\n",
    "best_last_similarity = 0\n",
    "best_index = -1\n",
    "\n",
    "for i, doc in enumerate(filtered_documents):\n",
    "    similarity = calc_similarity(i, filtered_documents, calculated_ngrams, 9)\n",
    "    \n",
    "    if similarity[-1][1] > best_last_similarity:\n",
    "        best_last_similarity = similarity[-1][1]\n",
    "        best_index = i\n",
    "\n",
    "best_doc_similarities = calc_similarity(best_index, filtered_documents, calculated_ngrams, 9)\n",
    "\n",
    "print(f'Best index: {best_index}. Related documents: {best_doc_similarities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "top officials of leading industrial nations appear deeply worried that financial markets have ignored their efforts to coordinate policies which they believe they strengthened in talks last week monetary sources said officials were exasperated that the markets which drove the dollar rapidly lower and severely disrupted bond and stock markets too did not take heed of the policy commitments of the group of seven the united states japan west germany france britain italy and canada treasury secretary james baker went out of his way to reassure markets of his commitment to a stable dollar with a statement and french finance minister edouard balladur underscored that by saying i do not believe at all that the americans want a weaker dollar west german finance minister gerhard stoltenberg said the dollar is latest rapid descent involves the risk now already a tangible threat of a new strong surge of inflation leading to a renewed rise in interest rates but there were signs too that while policymakers feared the market uproar they seemed to accept there was little they could do until the economic picture changed and currencies settled into a stable pattern as a result nor did there seem to be any enthusiasm at last week is semi annual meetings of the imf and the world bank for higher u s interest rates as the best way to curb the dollar is rapid descent that distaste stems in part from fears of recession outgoing deputy treasury secretary richard darman told television interviewers he did not think a policy of driving the dollar down would solve the u s trade deficit it would slow growth in germany and japan which would adversely affect our trade balance and ultimately it would drive interest rates up here which would throw us if not into recession into slower growth he said asked if higher u s interest rates would stabilize the dollar balladur said when a currency is maintained artificially high by artificially high interest rates it is not healthy and resorting to higher interest rates could lead to recession he said acknowledging the dollar is latest slide was now a fact of life balladur said there may be adjustments of course in one or other currencies this is not a fixed rate system but federal reserve board chairman paul volcker said he might rein in credit if the dollar is slide deepens u s monetary sources also said washington wanted it understood by markets the seven is commitments were genuine the united states and the six major industrial countries are fully committed to implementing our undertakings in these agreements baker told the meetings darman said baker had been misinterpreted by markets which wrongly believed earlier remarks suggested he wanted a further decline in the dollar baker darman said was committed to stabilizing currencies at current levels last week is statement from the seven reaffirmed a february 22 agreement in paris in which the reagan administration agreed to reach a budget deficit compromise with congress and to fight protectionism west germany and japan meanwhile agreed to stimulate domestic demand and lead a global upturn ministers believed the paris pact was bolstered by japan is promise of a 35 billion dlr supplementary budget the sources said they believed baker saw it as a major action but the seven seem to accept their commitment to stable currencies applied to today is exchange rates and not those at the time of the paris agreement when the dollar stood higher the paris accord said currencies are within ranges broadly consistent with underlying economic fundamentals given the policy commitments summarized in this statement now they accept the dollar is lower level especially against the yen as hard reality that is nonetheless consistent with the agreement the ministers and governors reaffirmed the view that around current levels their currencies are within ranges broadly consistent with fundamentals last week is statement read monetary sources said policymakers understood markets were focusing on instability created by the gap between the u s trade deficit and the surpluses of west germany and japan rather than prospective policy changes european monetary sources said bonn was still unconvinced that washington meant business with its commitment to cut the budget deficit reuter \n"
     ]
    }
   ],
   "source": [
    "# Best documents:\n",
    "# 173, 175, 183, 198, 125, 62, 197, 109, 148, 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[173, 175, 183, 198, 125, 62, 197, 109, 148, 102]\n"
     ]
    }
   ],
   "source": [
    "best_related_documents = [best_index]\n",
    "\n",
    "for doc in best_doc_similarities:\n",
    "    best_related_documents.append(doc[0])\n",
    "\n",
    "print(best_related_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in best_related_documents:\n",
    "    with open(f'./documents/{doc_id}.txt', mode='w') as file:\n",
    "        file.write(filtered_documents[doc_id])"
   ]
  }
 ]
}